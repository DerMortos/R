# data_100 = rexp(100, rate = 1)
# qqnorm(data_100)
# hist(data_100)
# data_1000 = rexp(1000, rate = 1)
# qqnorm(data_1000)
# hist(data_1000)
data100 <- rexp(10000, rate = 1)
matrixData <- matrix(data100, nrow = 100, ncol = 100)
means.exp <- apply(matrixData, 1, mean)
mean(means.exp)
hist(means.exp)
data1000 <- rexp(100000, rate = 1)
matrixData2 <- matrix(data1000, nrow = 1000, ncol = 100)
means.exp <- apply(matrixData2, 1, mean)
mean(means.exp)
hist(means.exp)
# data_100 = rexp(100, rate = 1)
# qqnorm(data_100)
# hist(data_100)
# data_1000 = rexp(1000, rate = 1)
# qqnorm(data_1000)
# hist(data_1000)
data100 <- rexp(10000, rate = 1)
matrixData <- matrix(data100, nrow = 100, ncol = 100)
means.exp <- apply(matrixData, 1, mean)
mean(means.exp)
hist(means.exp)
data1000 <- rexp(100000, rate = 1)
matrixData2 <- matrix(data1000, nrow = 1000, ncol = 100)
means.exp <- apply(matrixData2, 1, mean)
mean(means.exp)
hist(means.exp)
# Chapter 3 Statistics
#figure 3.1
n_repl=1000
newlist0=replicate(n_repl,sample(0:9,10,replace=TRUE))
ybar0=colMeans(newlist0)
mean(ybar0)
hist(ybar0,scale="frequency")
sd(ybar0)
sd(ybar0)*sqrt(10)
CI1=c(mean(ybar0)-1*sd(ybar0),mean(ybar0)+1*sd(ybar0))
sum((ybar0>CI1[1])*(ybar0<CI1[2]))/length(ybar0)
CI2=c(mean(ybar0)-2*sd(ybar0),mean(ybar0)+2*sd(ybar0))
sum((ybar0>CI2[1])*(ybar0<CI2[2]))/length(ybar0)
## Cor between u_i and delta_i
cor(c(1,2,3,4),c(.1,.1,.4,.4))
##############
# section 3.4
library(readxl)
sleep <- read_excel("/Users/HerrNorb/Library/Mobile Documents/com~apple~CloudDocs/SP '21/STAT 3336 Sampling/R code and docs/sleep.xlsx")
setwd("C:/Users/Norb/BabuFrik/Personal UTRGV Classes/'21 .Spring/STAT 3336 Sampling/R code and docs")
setwd("C:/Users/Norb/BabuFrik/Personal UTRGV Classes/'21 .Spring/STAT 3336 Sampling/R code and docs")
# The database is attached to the R search path. This means that the database is searched by R
# when evaluating a variable, so objects in the database can be accessed by simply giving their names.
attach(sleep)
N=length(BrainWt)
sleep
# Chapter 3 Statistics
#figure 3.1
n_repl=1000
newlist0=replicate(n_repl,sample(0:9,10,replace=TRUE))
ybar0=colMeans(newlist0)
mean(ybar0)
hist(ybar0,scale="frequency")
sd(ybar0)
sd(ybar0)*sqrt(10)
CI1=c(mean(ybar0)-1*sd(ybar0),mean(ybar0)+1*sd(ybar0))
sum((ybar0>CI1[1])*(ybar0<CI1[2]))/length(ybar0)
CI2=c(mean(ybar0)-2*sd(ybar0),mean(ybar0)+2*sd(ybar0))
sum((ybar0>CI2[1])*(ybar0<CI2[2]))/length(ybar0)
## Cor between u_i and delta_i
cor(c(1,2,3,4),c(.1,.1,.4,.4))
##############
# section 3.4
library(readxl)
## on mac: /Users/HerrNorb/Library/Mobile Documents/com~apple~CloudDocs/SP '21/STAT 3336 Sampling/R code and docs/
setwd("C:/Users/Norb/BabuFrik/Personal UTRGV Classes/'21 .Spring/STAT 3336 Sampling/R code and docs/")
sleep <- read_excel("sleep.xlsx")
View(sleep)
# The database is attached to the R search path. This means that the database is searched by R
# when evaluating a variable, so objects in the database can be accessed by simply giving their names.
attach(sleep)
N=length(BrainWt)
hist(BrainWt,scale="frequency")
mean(BrainWt)
# Population standard deviation
sd(BrainWt)*sqrt((N-1)/N)
######
n_repl=100
n=40
newlist1=replicate(n_repl,sample(BrainWt,n,replace=FALSE))
ybar1=colMeans(newlist1)
hist(ybar1,scale="frequency")
######
n_repl=1000
n=5
LBW=log(BrainWt)
newlist2=replicate(n_repl,sample(LBW,n,replace=FALSE))
ybar2=colMeans(newlist2)
hist(ybar2,scale="frequency",xlab = "Mean of the logarithm of brain weight",main = "Sampling distribution")
mean(ybar2)
sd(ybar2)
mean(LBW)
sd(LBW)*sqrt((N-1)/N)
sd(LBW)*sqrt((N-1)/N)/sqrt(n)
CI1=c(mean(ybar2)-1*sd(ybar2),mean(ybar2)+1*sd(ybar2))
sum((ybar2>CI1[1])*(ybar2<CI1[2]))/length(ybar2)
CI2=c(mean(ybar2)-2*sd(ybar2),mean(ybar2)+2*sd(ybar2))
sum((ybar2>CI2[1])*(ybar2<CI2[2]))/length(ybar2)
###########################
## Box-Cox transformation
library(MASS)
bc<-boxcox(BrainWt~1,plotit=TRUE)
lambdabc<-bc$x[which.max(bc$y)]
n_repl=1000
n=5
BCBW=((BrainWt)^lambdabc -1)/lambdabc
newlist3=replicate(n_repl,sample(BCBW,n,replace=FALSE))
ybar3=colMeans(newlist3)
hist(ybar3,scale="frequency",xlab = "Mean of the Box-Cox of brain weight",main = "Sampling distribution")
CI1=c(mean(ybar3)-1*sd(ybar3),mean(ybar3)+1*sd(ybar3))
sum((ybar3>CI1[1])*(ybar3<CI1[2]))/length(ybar3)
CI2=c(mean(ybar3)-2*sd(ybar3),mean(ybar3)+2*sd(ybar3))
sum((ybar3>CI2[1])*(ybar3<CI2[2]))/length(ybar3)
qqnorm(BCBW)
qqline(BCBW)
shapiro.test(BCBW)
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
plot(pressure)
n_repl <- 1000
newlist0 <- replicate(n_repl, sample(0:9, 10, replace = TRUE))
ybar0 <- colMeans(newlist0)
mean_ybar0 <- mean(ybar0)
hist(ybar0, scale = "frequency")
sd_ybar0 <- sd(ybar0)
adjusted_sd <- sd_ybar0 * sqrt(10)
mean_ybar0
sd_ybar0
adjusted_sd
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
plot(pressure)
n_repl <- 1000
newlist0 <- replicate(n_repl, sample(0:9, 10, replace = TRUE))
ybar0 <- colMeans(newlist0)
mean_ybar0 <- mean(ybar0)
hist(ybar0, scale = "True")
sd_ybar0 <- sd(ybar0)
adjusted_sd <- sd_ybar0 * sqrt(10)
mean_ybar0
sd_ybar0
adjusted_sd
# 3.4 Sampling Distribution
# Reading and viewing data from an Excel file
library(readxl)
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
plot(pressure)
n_repl <- 1000
newlist0 <- replicate(n_repl, sample(0:9, 10, replace = TRUE))
ybar0 <- colMeans(newlist0)
mean_ybar0 <- mean(ybar0)
hist(ybar0, freq = TRUE)
sd_ybar0 <- sd(ybar0)
adjusted_sd <- sd_ybar0 * sqrt(10)
mean_ybar0
sd_ybar0
adjusted_sd
hist(ybar3,freq = TRUE,xlab = "Mean of the Box-Cox transformed brain weight",main = "Sampling distribution")
# Chapter 3 Statistics
# 3.2 Summarizing Information in Populations and Samples: Infinite Population Case
# figure 3.1
# Simulating the sampling distribution of the sample mean
n_repl=1000                                             # Number of replications
newlist0=replicate(n_repl,sample(0:9,10,replace=TRUE))  # Replicating the sample 1000 times
ybar0=colMeans(newlist0)                                # Calculating the mean of each sample
mean(ybar0)                                             # Calculating the overall mean of the sample means
hist(ybar0,freq = TRUE)                                 # Creating a histogram of the sample means
sd(ybar0)                                               # Calculating the sd of the sample means
sd(ybar0)*sqrt(10)                                      # Adjusting the sd for the sample size
# Calculating confidence intervals (CI) and their coverage
CI1=c(mean(ybar0)-1*sd(ybar0),mean(ybar0)+1*sd(ybar0))  # 68% CI
sum((ybar0>CI1[1])*(ybar0<CI1[2]))/length(ybar0)        # Proportion of means within the CI
CI2=c(mean(ybar0)-2*sd(ybar0),mean(ybar0)+2*sd(ybar0))  # 95% CI
sum((ybar0>CI2[1])*(ybar0<CI2[2]))/length(ybar0)        # Proportion of means within the CI
## Correlation between two sets of numbers
cor(c(1,2,3,4),c(.1,.1,.4,.4)) # Computes the correlation between the two vectors
##############
##############
# 3.4 Sampling Distribution
# Reading and viewing data from an Excel file
library(readxl)
setwd("C:/Users/Norb/BabuFrik/Personal UTRGV Classes/'21 .Spring/STAT 3336 Sampling/R code and docs/")
sleep <- read_excel("sleep.xlsx")
View(sleep)
# Attaching the database to the R search path
attach(sleep) # This allows you to refer to the variables in sleep directly by name
# Population size and summary statistics
N=length(BrainWt) # Number of observations for brain weight
hist(BrainWt,freq = TRUE) # Histogram of brain weight
mean(BrainWt) # Mean of brain weight
# Population sd, adjusted for the finite population correction
sd(BrainWt)*sqrt((N-1)/N)
######
# Sampling distribution of the sample mean for brain weight
n_repl=100                            # Number of replications
n=40                                  # Sample size
newlist1=replicate(n_repl,sample(BrainWt,n,replace=FALSE)) # Replicating the sample 100 times
ybar1=colMeans(newlist1)              # Calculating the mean of each sample
hist(ybar1,freq = TRUE)         # Creating a histogram of the sample means
######
# Log transformation of the data
n_repl=1000                           # Number of replications
n=5                                   # Sample size
LBW=log(BrainWt)                      # Log transformation of brain weights
newlist2=replicate(n_repl,sample(LBW,n,replace=FALSE)) # Replicating the log-transformed samples
ybar2=colMeans(newlist2)              # Calculating the mean of each log-transformed sample
hist(ybar2,freq = TRUE,xlab = "Mean of the logarithm of brain weight",main = "Sampling distribution")
mean(ybar2)                           # Mean of the sample means
sd(ybar2)                             # Sd of the sample means
# Confidence intervals for log-transformed data
mean(LBW)                             # Mean of the log-transformed brain weights
sd(LBW)*sqrt((N-1)/N)                 # Sd, adjusted for the finite population correction
sd(LBW)*sqrt((N-1)/N)/sqrt(n)         # Standard error of the sample mean
CI1=c(mean(ybar2)-1*sd(ybar2),mean(ybar2)+1*sd(ybar2)) # 68% confidence interval for log-transformed data
CI2=c(mean(ybar2)-2*sd(ybar2),mean(ybar2)+2*sd(ybar2)) # 95% confidence interval for log-transformed data
###########################
## Box-Cox transformation
# Using the Box-Cox transformation to normalize data
library(MASS)
bc<-boxcox(BrainWt~1,plotit=TRUE)     # Performing Box-Cox transformation and plotting the likelihood
lambdabc<-bc$x[which.max(bc$y)]       # Finding the lambda that maximizes the log-likelihood
# Sampling distribution of the Box-Cox transformed sample means
n_repl=1000                           # Number of replications
n=5                                   # Sample size
BCBW=((BrainWt)^lambdabc -1)/lambdabc # Applying the Box-Cox transformation
newlist3=replicate(n_repl,sample(BCBW,n,replace=FALSE))   # Replicating the Box-Cox transformed samples
ybar3=colMeans(newlist3)              # Calculating the mean of each transformed sample
hist(ybar3,freq = TRUE,xlab = "Mean of the Box-Cox transformed brain weight",main = "Sampling distribution")
# Confidence intervals for Box-Cox transformed data
CI1=c(mean(ybar3)-1*sd(ybar3),mean(ybar3)+1*sd(ybar3)) # 68% confidence interval for Box-Cox data
CI2=c(mean(ybar3)-2*sd(ybar3),mean(ybar3)+2*sd(ybar3)) # 95% confidence interval for Box-Cox data
# Normality tests for Box-Cox transformed data
qqnorm(BCBW)                          # Quantile-Quantile plot for normality check
qqline(BCBW)                          # Adding a line to the Q-Q plot for reference
shapiro.test(BCBW) # Performing Shapiro-Wilk test for normality
# Chapter 3 Statistics
# 3.2 Summarizing Information in Populations and Samples: Infinite Population Case
# figure 3.1
# Simulating the sampling distribution of the sample mean
n_repl=1000                                             # Number of replications
newlist0=replicate(n_repl,sample(0:9,10,replace=TRUE))  # Replicating the sample 1000 times
ybar0=colMeans(newlist0)                                # Calculating the mean of each sample
mean(ybar0)                                             # Calculating the overall mean of the sample means
hist(ybar0,freq = TRUE)                                 # Creating a histogram of the sample means
sd(ybar0)                                               # Calculating the sd of the sample means
sd(ybar0)*sqrt(10)                                      # Adjusting the sd for the sample size
# Calculating confidence intervals (CI) and their coverage
CI1=c(mean(ybar0)-1*sd(ybar0),mean(ybar0)+1*sd(ybar0))  # 68% CI
sum((ybar0>CI1[1])*(ybar0<CI1[2]))/length(ybar0)        # Proportion of means within the CI
CI2=c(mean(ybar0)-2*sd(ybar0),mean(ybar0)+2*sd(ybar0))  # 95% CI
sum((ybar0>CI2[1])*(ybar0<CI2[2]))/length(ybar0)        # Proportion of means within the CI
## Correlation between two sets of numbers
cor(c(1,2,3,4),c(.1,.1,.4,.4)) # Computes the correlation between the two vectors
##############
##############
# 3.4 Sampling Distribution
# Reading and viewing data from an Excel file
library(readxl)
setwd("C:/Users/Norb/BabuFrik/Personal UTRGV Classes/'21 .Spring/STAT 3336 Sampling/R code and docs/")
sleep <- read_excel("sleep.xlsx")
View(sleep)
# Attaching the database to the R search path
attach(sleep) # This allows you to refer to the variables in sleep directly by name
# Population size and summary statistics
N=length(BrainWt) # Number of observations for brain weight
hist(BrainWt,freq = TRUE) # Histogram of brain weight
mean(BrainWt) # Mean of brain weight
# Population sd, adjusted for the finite population correction
sd(BrainWt)*sqrt((N-1)/N)
######
# Sampling distribution of the sample mean for brain weight
n_repl=100                            # Number of replications
n=40                                  # Sample size
newlist1=replicate(n_repl,sample(BrainWt,n,replace=FALSE)) # Replicating the sample 100 times
ybar1=colMeans(newlist1)              # Calculating the mean of each sample
hist(ybar1,freq = TRUE)         # Creating a histogram of the sample means
######
# Log transformation of the data
n_repl=1000                           # Number of replications
n=5                                   # Sample size
LBW=log(BrainWt)                      # Log transformation of brain weights
newlist2=replicate(n_repl,sample(LBW,n,replace=FALSE)) # Replicating the log-transformed samples
ybar2=colMeans(newlist2)              # Calculating the mean of each log-transformed sample
hist(ybar2,freq = TRUE,xlab = "Mean of the logarithm of brain weight",main = "Sampling distribution")
mean(ybar2)                           # Mean of the sample means
sd(ybar2)                             # Sd of the sample means
# Confidence intervals for log-transformed data
mean(LBW)                             # Mean of the log-transformed brain weights
sd(LBW)*sqrt((N-1)/N)                 # Sd, adjusted for the finite population correction
sd(LBW)*sqrt((N-1)/N)/sqrt(n)         # Standard error of the sample mean
CI1=c(mean(ybar2)-1*sd(ybar2),mean(ybar2)+1*sd(ybar2)) # 68% confidence interval for log-transformed data
CI2=c(mean(ybar2)-2*sd(ybar2),mean(ybar2)+2*sd(ybar2)) # 95% confidence interval for log-transformed data
###########################
## Box-Cox transformation
# Using the Box-Cox transformation to normalize data
library(MASS)
bc<-boxcox(BrainWt~1,plotit=TRUE)     # Performing Box-Cox transformation and plotting the likelihood
lambdabc<-bc$x[which.max(bc$y)]       # Finding the lambda that maximizes the log-likelihood
# Sampling distribution of the Box-Cox transformed sample means
n_repl=1000                           # Number of replications
n=5                                   # Sample size
BCBW=((BrainWt)^lambdabc -1)/lambdabc # Applying the Box-Cox transformation
newlist3=replicate(n_repl,sample(BCBW,n,replace=FALSE))   # Replicating the Box-Cox transformed samples
ybar3=colMeans(newlist3)              # Calculating the mean of each transformed sample
hist(ybar3,freq = TRUE,xlab = "Mean of the Box-Cox transformed brain weight",main = "Sampling distribution")
# Confidence intervals for Box-Cox transformed data
CI1=c(mean(ybar3)-1*sd(ybar3),mean(ybar3)+1*sd(ybar3)) # 68% confidence interval for Box-Cox data
CI2=c(mean(ybar3)-2*sd(ybar3),mean(ybar3)+2*sd(ybar3)) # 95% confidence interval for Box-Cox data
# Normality tests for Box-Cox transformed data
qqnorm(BCBW)                          # Quantile-Quantile plot for normality check
qqline(BCBW)                          # Adding a line to the Q-Q plot for reference
shapiro.test(BCBW) # Performing Shapiro-Wilk test for normality
setwd("C:/Users/Norb/BabuFrik/programming/R/Experimental Design and Sampling/3336 Sampling")
